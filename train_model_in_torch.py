# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1b23I0KUZhYcvUjIm11Tq1ppCYkDw0mQR
"""

!pip3 install torch torchvision torchaudio

import torch
x = torch.rand(1)
print(x)

y = torch.rand(2)
print(y)

a = torch.rand(2,3)
print(a)

b = torch.rand(2,3,4)
print(b)
print(b.ndim," Dimensions")

print(b.size())
print(b.shape)

x = torch.zeros(5,3, dtype=torch.float16)
print(x)

# this will tell pytorch that it will need to calculate the gradients for this tensor
x = torch.tensor([5.5,3], requires_grad=True)
print(x)

x = torch.rand(3,4)
print(x)
print(x[:, 1])
print(x[1, :])
print(x[:,-1])
print(x[1,1])
print(x[1,1].item())

x = torch.randn(4,4)
y = x.view(16)
z = x.view(-1,8)
# if -1 then pytorch automatically determine the necessary size
print(x, y, z)
print(x.size(), y.size(), z.size())

"""## 2. Autograd

The autograd package provides automatic differentiation for all operations on Tensors. Generally speaking, *torch.autograd* is an engine for computing the vector-Jacobian product. It computes partial derivates while applying the chain rule.

Set `requires_grad = True`:
"""

x = torch.rand(2, requires_grad=True)
y = x + 2

print(x)
print(y)
print(y.grad_fn)

z = y*y*3
print(z)
z = z.mean()
print(z)

"""#### Stop a tensor from tracking history:
For example during the training loop when we want to update our weights, or after training during evaluation. These operations should not be part of the gradient computation. To prevent this, we can use:

- `x.requires_grad_(False)`
- `x.detach()`
- wrap in `with torch.no_grad():`
"""

# Let's compute the gradients with backpropagation
# When we finish our computation we can call .backward() and have all the gradients computed automatically.
# The gradient for this tensor will be accumulated into .grad attribute.
# It is the partial derivate of the function w.r.t. the tensor

print(x.grad)
z.backward()
print(x.grad)

# !!! Careful!!! backward() accumulates the gradient for this tensor into .grad attribute.
# !!! We need to be careful during optimization !!! optimizer.zero_grad()

"""## Gradient Descent Autograd
Linear Regression example:

$f(x) = w * x + b$

here : `f(x) = 2 * x`
"""

import torch 

X = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8], dtype=torch.float32)
Y = torch.tensor([2, 4, 6, 8, 10, 12, 14, 16], dtype=torch.float32)

w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)

def forward(x):
  return w*x

x_test = 5.0
print(f"prediction before training: f({x_test}) = {forward(x_test).item():.3f}")

w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)
print(type(w.grad))

# training
learning_rate = 0.01
n_epochs=100

for epochs in range(n_epochs):
  y_pred = forward(X)

  l = ((y_pred-Y)**2).mean()
  print(l)

  l.backward()

  #update weight
  with torch.no_grad():
    w -= learning_rate * w.grad

  # zero gradient after updating
  w.grad.zero_()

  if (epochs+1)%10 == 0:
    print(f"epoch: {epochs+1}: w={w.item():.3f}, loss= {l.item():.3f}")

print(f"prediction after training: f({x_test}) = {forward(x_test).item():.3f}")

"""## 3. Model, Loss & Optimizer

A typical PyTorch pipeline looks like this:

1. Design model (input, output, forward pass with different layers)
2. Construct loss and optimizer
3. Training loop:
  - Forward = compute prediction and loss
  - Backward = compute gradients
  - Update weights
"""

import torch
import torch.nn as nn
X = torch.tensor([[1], [2], [3], [4], [5], [6], [7], [8]], dtype=torch.float32)
Y = torch.tensor([[2], [4], [6], [8], [10], [12], [14], [16]], dtype=torch.float32)

n_sample, n_feature = X.shape
print(f"n_sample: {n_sample}, n_feature: {n_feature}")

X_test = torch.tensor([5], dtype=torch.float32)

class LinearRegression(nn.Module):
  def __init__(self, input_dim, output_dim):
    super(LinearRegression, self).__init__()
    self.lin = nn.Linear(input_dim, output_dim)

  def forward(self, x):
    return self.lin(x)

input_size, output_size = n_feature, n_feature
model = LinearRegression(input_size, output_size)
print(f"prediction before training f({X_test.item()}) = {model(X_test).item():.3f}")

learning_rate = 0.01
n_epochs = 100

loss = nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)

for epochs in range(n_epochs):
  y_prediction = model(X)

  l = loss(Y, y_prediction)

  l.backward()

  optimizer.step()

  optimizer.zero_grad()

  if (epochs+1) % 10 == 0:
    w, b = model.parameters()
    print("epochs ", epochs+1, ": w=", w[0][0].item(), "loss = ", l.item())

print(f"prediction after training: f({X_test.item()}) = {model(X_test).item():.3f}")

import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

input_size = 784
hidden_state = 500
num_class = 10
num_epochs = 2
batch_size = 100
learning_rate = 0.001

train_dataset = torchvision.datasets.MNIST(root='./data',
                                           train = True,
                                           transform = transforms.ToTensor(),
                                           download=True)
test_dataset = torchvision.datasets.MNIST(root='./data',
                                          train=False,
                                          transform = transforms.ToTensor())

train_loader = torch.utils.data.DataLoader(dataset=train_dataset,
                                               batch_size = batch_size,
                                               shuffle=False)
test_loader = torch.utils.data.DataLoader(dataset=test_dataset,
                                               batch_size = batch_size,
                                               shuffle=False)

# now wo iterate over dataloader we can use for loop or iter method
examples = iter(test_loader)
example_data, example_target = next(examples)

for i in range(6):
  plt.subplot(2,3,i+1)
  plt.imshow(example_data[i][0], cmap='gray')
plt.show()

class NeuralNet(nn.Module):
  def __init__(self, input_size, hidden_state, num_class):
    super(NeuralNet, self).__init__()
    self.l1 = nn.Linear(input_size, hidden_state)
    self.relu = nn.ReLU()
    self.l2 = nn.Linear(hidden_state, num_class)

  def forward(self, x):
    out = self.l1(x)
    out = self.relu(out)
    out = self.l2(out)
    return out

model = NeuralNet(input_size, hidden_state, num_class).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

n_total_steps = len(train_loader)
for epochs in range(num_epochs):
  for i,(images, labels) in enumerate(train_loader):
    images = images.reshape(-1,28*28).to(device)
    labels = labels.to(device)

    output = model(images)
    loss = criterion(output, labels)

    loss.backward()
    optimizer.step()
    optimizer.zero_grad()

    if (i+1) % 100 == 0:
      print(f"epochs: [{epochs+1}/{num_epochs}], step[{i+1}/{n_total_steps}], loss:{loss.item():.3f}")

with torch.no_grad():
  n_correct = 0
  n_samples = len(test_loader.dataset)

  for images, labels in test_loader:
    images = images.reshape(-1, 28*28).to(device)
    labels = labels.to(device)

    output = model(images)

    _,prediction = torch.max(output, 1)
    n_correct += (prediction==labels).sum().item()

    acc = n_correct/n_samples

    print(f"accuracy of the network {n_samples} test_image: {100*acc} %")

